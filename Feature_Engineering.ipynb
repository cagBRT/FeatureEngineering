{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPMfvdD7Afec3NQjA7ooFft",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/FeatureEngineering/blob/master/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fymCvydgpk3p"
      },
      "source": [
        "https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1mS0e1644qO"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JqoEbWsSP0f"
      },
      "source": [
        "!pip3 install impyute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIBzUCqb6hjw"
      },
      "source": [
        "!pip3 install datawig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSnfJ2MPpDKM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from sklearn.impute import SimpleImputer\n",
        "import random\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import sys\n",
        "from sklearn.impute import KNNImputer\n",
        "import impyute as impy\n",
        "from impyute.imputation.cs import fast_knn\n",
        "from impyute.imputation.cs import mice\n",
        "import datawig\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvZQpaqS-B8R"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/FeatureEngineering.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTuNjy718xqi"
      },
      "source": [
        "boston = load_boston()\n",
        "x = boston.data\n",
        "y = boston.target\n",
        "columns = boston.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmkmSltq9OVE"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"boston_housing.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsoxk3rx87ou"
      },
      "source": [
        "#create the dataframe\n",
        "boston_df = pd.DataFrame(boston.data)\n",
        "boston_df.columns = columns\n",
        "boston_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wYy8lNc0P5E"
      },
      "source": [
        "# **Know Your Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ASQWDaVIsi"
      },
      "source": [
        "boston_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsxVj7gAcjrU"
      },
      "source": [
        "boston_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNy45Qc8dQl8"
      },
      "source": [
        "boston_data_stats = boston_df.describe()\n",
        "boston_data_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmmlgzDfvLpE"
      },
      "source": [
        "boston_df[\"ZN\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jvs39rFd5OT"
      },
      "source": [
        "# **Handling Outliers**\n",
        "\n",
        "In statistics, an outlier is an observation point that is distant from other observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbJkGpowmudp"
      },
      "source": [
        "Machine learning algorithms are susceptible to the statistics and distribution of the input variables. \n",
        "\n",
        "Data outliers can spoil and mislead the training process resulting in longer training times, less accurate models, and, ultimately, more mediocre results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWoMK-sa0bWn"
      },
      "source": [
        "**Identify Outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeJhm_ri05p6"
      },
      "source": [
        "**BoxPlot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFP6jT8m4ON"
      },
      "source": [
        "One of the simplest methods for detecting outliers is the use of box plots. A box plot is a graphical display for describing the distributions of the data. Box plots use the median and the lower and upper quartiles.<br><br>\n",
        "An outlier as those values of a variable that fall far from the central point, the median."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZByx54qEvA"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"boxplots.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s0OkusHd6FF"
      },
      "source": [
        "sns.boxplot(y=boston_df['ZN'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkPlcOJUoUD4"
      },
      "source": [
        "sns.boxplot(y=boston_df['AGE'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnLC68s0uQb"
      },
      "source": [
        "**Scatter Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx26M8HZeuSO"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.scatter(boston_df['CRIM'], boston_df['AGE'])\n",
        "ax.set_xlabel('Crime')\n",
        "ax.set_ylabel('Age')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQJwYT4QrjjE"
      },
      "source": [
        "Outliers can either be a mistake or just data variance, how would you decide if they are important or not. Well, it is pretty simple if they are the result of a mistake, then we can ignore them, but if it is just a variance in the data we would need think a bit further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPX6CibR03NJ"
      },
      "source": [
        "# **Z Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi0_JSzrfbeg"
      },
      "source": [
        "Discover outliers with mathematical function\n",
        "Z-Score-<br>\n",
        "The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.<br>\n",
        "The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.<br>\n",
        "While calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These **data points which are way too far from zero will be treated as the outliers**. <br>\n",
        "In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ysHPFQy2Y95"
      },
      "source": [
        "Image(\"ZScore.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCFKcHonfcWE"
      },
      "source": [
        "z = np.abs(stats.zscore(boston_df))\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyjTT9WCpnr3"
      },
      "source": [
        "The first array contains the list of row numbers and second array respective column numbers, which mean z[55][1] have a Z-score higher than 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPOkQO6sfxHG"
      },
      "source": [
        "threshold = 3\n",
        "print(np.where(z > 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XY3A-6K2uDL"
      },
      "source": [
        "55th record on column ZN is an outlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7FXMWzUpubg"
      },
      "source": [
        "print(z[55][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HCyrjh921_W"
      },
      "source": [
        "**IQR Score**<br>\n",
        "Box plots use the IQR (interquartile range) method to display data and outliers(shape of the data) but in order to get a list of identified outliers, we need to use the mathematical formula and retrieve the outlier data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKGr4hG1ATad"
      },
      "source": [
        "sns.boxplot(y=boston_df['INDUS'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVUkWCJU25Lo"
      },
      "source": [
        "Q1 = boston_df.quantile(0.25)\n",
        "Q3 = boston_df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(IQR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWl-s_5I3afm"
      },
      "source": [
        "With IQR scores, you can now get the outliers. The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas **True indicates presence of an outlier**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T1GcyR23bZP"
      },
      "source": [
        "outliers = (boston_df < (Q1 - 1.5 * IQR)) |(boston_df > (Q3 + 1.5 * IQR))\n",
        "print(outliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jf9pOEZB-Jd"
      },
      "source": [
        "print(np.where(outliers == True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRcWYkjBPKy"
      },
      "source": [
        "outliers.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJGrNtc5IC0"
      },
      "source": [
        "**Using the Z Score to Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTkta-KU5DU6"
      },
      "source": [
        "print(boston_df.shape)\n",
        "boston_df = boston_df[(z < 3).all(axis=1)]\n",
        "print(boston_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8MXSnun6Dug"
      },
      "source": [
        "**Using the IQR Score to Remove Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQP_KpL6I8Y"
      },
      "source": [
        "boston_df_out = boston_df[~((boston_df < (Q1 - 1.5 * IQR)) |(boston_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "boston_df_out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLtoD9E79_Bo"
      },
      "source": [
        "https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtpemhD60ah"
      },
      "source": [
        "**Whether an outlier should be removed or not**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZMBKKWN7Fxu"
      },
      "source": [
        "An outlier may indicate bad data. <br>\n",
        ">For example, the data may have been coded incorrectly or an experiment may not have been run correctly. <br>\n",
        "\n",
        "If it can be determined that an outlying point is in fact erroneous, then the outlying value should be deleted from the analysis (or corrected if possible).<br>\n",
        "\n",
        "In some cases, it may not be possible to determine if an outlying point is bad data. <br>\n",
        "\n",
        "Outliers may be due to random variation or may indicate something scientifically interesting. In any event, **do not simply delete the outlying observation before a through investigation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJgbzoz7O4M"
      },
      "source": [
        "**Outliers should be rare. If they are not rare, the method (and hence the entire data set) is bad and/or not trustworthy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwTn9Spi7j-3"
      },
      "source": [
        "If outliers are rare, they have no statistical impact. In small samples they will be extremely rare (what is not a statistical problem, although they may have a considerable impact these particular case where they in fact occur), in large samples they won't have any considerable levarage or impact - so why care?\n",
        "In small samples there is another \"problem\": values may be outlying - but not because these outlying values are \"wrong\" but rather because the rest of the values cumps together more tightly as they should. So the \"outlier\" is actually the only datum \"putting things right\". Removing it would unneccesarily introduce bias, what is a rather bad thing.\n",
        "Clearly, outliers with considerable leavarage can indicate a problem with the measurement or the data recording, communication or whatever. In *such* cases it is absolutely recommended to remove these values. But the judgement about this is based on reasons external to the data. Either the values are known to be \"impossible\" (e.g. recordings of a persons weight of 87653 kg [the mistake could be that the weight was wrongly given in grams] or the duration of a hostpital stay of more than 153 years and such). Other outlying but not-impossible values might be caused by special circumstances, like a disease study subject, the change of the operator (because the original operator was sick that day the suspect measurement was recorded), a power failure, something like this. But these resons have to be identified to know if the removal of the outlying value would improve the results or possible introduce bias. It may not alway be so easy to find the reasons, especially when looking at multivariate outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDVKdo_m7zoj"
      },
      "source": [
        "Bad data, wrong calculation, these can be identified as Outliers and should be dropped but at the same time you might want to correct them too, as they change the level of data i.e. mean which cause issues when you model your data. <br>\n",
        "**For example**:<br>\n",
        "5 people get salary of 10K, 20K, 30K, 40K and 50K and suddenly one of the person start getting salary of 100K. <br>\n",
        "Consider this situation as, you are the employer, the new salary update might be seen as biased and you might need to increase other employee’s salary too, to keep the balance. So, there can be multiple reasons you want to understand and correct the outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkEgcbtU9LtH"
      },
      "source": [
        "Dropping data is always a extreme step and should be taken only in extreme conditions when we’re very sure that the outlier is a measurement error, which we generally do not know. <br><br>\n",
        "When we drop data, we lose information in terms of the variability in data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Ke70EJTJ0y"
      },
      "source": [
        "# **Imputation**\n",
        "\n",
        "One way to handle missing data is to get rid of the observations that have missing data. However, you will risk losing data points with valuable information. A better strategy would be to impute the missing values. In other words, infer the missing values from the existing part of the data. There are three main types of missing data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlYJ4DljZ0R_"
      },
      "source": [
        "https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xjgzs-GYz2m"
      },
      "source": [
        "**Types of Missing Data**<br>\n",
        "Missing completely at random (MCAR)\n",
        "Missing at random (MAR)\n",
        "Not missing at random (NMAR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzie1_S_ZQlD"
      },
      "source": [
        "Some algorithms can factor in the missing values and learn the best imputation values for the missing data based on the training loss reduction (ie. XGBoost). Some others have the option to just ignore them (ie. LightGBM — use_missing=false). However, other algorithms will panic and throw an error complaining about the missing values (ie. Scikit learn — LinearRegression). In that case, you will need to handle the missing data and clean it before feeding it to the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNvJmQQRzax"
      },
      "source": [
        "**Numerical Imputation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltfBSYaHSJiV"
      },
      "source": [
        "#Create a dataset with missing values\n",
        "dataset = fetch_california_housing()\n",
        "#Fetching the dataset\n",
        "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
        "train.columns = ['0','1','2','3','4','5','6','7']\n",
        "train.insert(loc=len(train.columns), column='target', value=target)\n",
        "#Randomly replace 40% of the first column with NaN values\n",
        "column = train['0']\n",
        "print(column.size)\n",
        "missing_pct = int(column.size * 0.8)\n",
        "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
        "column[i] = np.NaN\n",
        "print(column.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfGCdVg_VTcM"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JrSq8UVUWAf"
      },
      "source": [
        "train.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc39L1NbYxSy"
      },
      "source": [
        "Fill the missing data with the mean of the column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRppmoxbSVAB"
      },
      "source": [
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imp_mean.fit(train)\n",
        "imputed_train_df = imp_mean.transform(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNOBK-bwZavg"
      },
      "source": [
        "Notice the mean for train[0] = 3.8412699"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVISaDtTg1f"
      },
      "source": [
        "imputed_train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd6wj8r1ZnvK"
      },
      "source": [
        "Calculating the mean/median of the non-missing values in a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT_RtEoqpXDw"
      },
      "source": [
        "**Numerical Imputation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7erH99SsaIGD"
      },
      "source": [
        "Pros:\n",
        ">Easy and fast.\n",
        "<br>\n",
        "\n",
        ">Works well with small numerical datasets.\n",
        "\n",
        "Cons:\n",
        ">Doesn’t factor the correlations between features. It only works on the column level.\n",
        "Will give poor results on encoded categorical features (do NOT use it on categorical features).\n",
        "Not very accurate.\n",
        "Doesn’t account for the uncertainty in the imputations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8e2ftciamLL"
      },
      "source": [
        "**Imputation Using (Most Frequent)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4BSTWReatSV"
      },
      "source": [
        "Most Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features (strings or numerical representations) by replacing missing data with the most frequent values within each column.\n",
        "Pros:\n",
        "Works well with categorical features.\n",
        "Cons:\n",
        "It also doesn’t factor the correlations between features.\n",
        "It can introduce bias in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dRHsDQDN9jZ"
      },
      "source": [
        "#Create a dataset with missing values\n",
        "dataset = fetch_california_housing()\n",
        "#Fetching the dataset\n",
        "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
        "train.columns = ['0','1','2','3','4','5','6','7']\n",
        "train.insert(loc=len(train.columns), column='target', value=target)\n",
        "#Randomly replace 40% of the first column with NaN values\n",
        "column = train['0']\n",
        "print(column.size)\n",
        "missing_pct = int(column.size * 0.8)\n",
        "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
        "column[i] = np.NaN\n",
        "print(column.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3_EwQRKbkk6"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm5c81B8az__"
      },
      "source": [
        "#Impute the values using scikit-learn SimpleImpute Class\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp_mean = SimpleImputer( strategy='most_frequent')\n",
        "imp_mean.fit(train)\n",
        "imputed_train_df = imp_mean.transform(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgr9ChcSbbNK"
      },
      "source": [
        "Note the most frequent value in column '0' is 2.875\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWDY2zpnaVzS"
      },
      "source": [
        "imputed_train_df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qcui2XqbSrg"
      },
      "source": [
        "**Imputation Using  (Zero/Constant) Values**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMibWp6ibf-1"
      },
      "source": [
        "Pros:\n",
        "Works well with categorical features.\n",
        "Cons:\n",
        "It also doesn’t factor the correlations between features.\n",
        "It can introduce bias in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAaQgbMOb945"
      },
      "source": [
        "#Create a dataset with missing values\n",
        "dataset = fetch_california_housing()\n",
        "#Fetching the dataset\n",
        "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
        "train.columns = ['0','1','2','3','4','5','6','7']\n",
        "train.insert(loc=len(train.columns), column='target', value=target)\n",
        "#Randomly replace 40% of the first column with NaN values\n",
        "column = train['0']\n",
        "print(column.size)\n",
        "missing_pct = int(column.size * 0.8)\n",
        "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
        "column[i] = np.NaN\n",
        "print(column.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mfk7uu_cHkc"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rre3-YepWSc"
      },
      "source": [
        "#Filling all missing values with 0\n",
        "data = train.fillna(0)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE8heOD7cQ0h"
      },
      "source": [
        "#Create a dataset with missing values\n",
        "dataset = fetch_california_housing()\n",
        "#Fetching the dataset\n",
        "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
        "train.columns = ['0','1','2','3','4','5','6','7']\n",
        "train.insert(loc=len(train.columns), column='target', value=target)\n",
        "#Randomly replace 40% of the first column with NaN values\n",
        "column = train['0']\n",
        "print(column.size)\n",
        "missing_pct = int(column.size * 0.8)\n",
        "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
        "column[i] = np.NaN\n",
        "print(column.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWvpf7q0cdDH"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA0dQaT2cCMe"
      },
      "source": [
        "#Filling missing values with medians of the columns\n",
        "data = train.fillna(train.median())\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFCLOnqvcGKk"
      },
      "source": [
        "**Imputation Using k-NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcg6b7aEcMXg"
      },
      "source": [
        "The k nearest neighbours is an algorithm that is used for simple classification. The algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood. Let’s see some example code using Impyute library which provides a simple and easy way to use KNN for imputation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtcQ4b8cgcL"
      },
      "source": [
        "It creates a basic mean impute then uses the resulting complete list to construct a KDTree. Then, it uses the resulting KDTree to compute nearest neighbours (NN). After it finds the k-NNs, it takes the weighted average of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXCekrR9cc75"
      },
      "source": [
        "Pros:\n",
        "Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n",
        "Cons:\n",
        "Computationally expensive. KNN works by storing the whole training dataset in memory.\n",
        "K-NN is quite sensitive to outliers in the data (unlike SVM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFrvVxTWcPeZ"
      },
      "source": [
        "sys.setrecursionlimit(100000) #Increase the recursion limit of the OS\n",
        "\n",
        "# start the KNN training\n",
        "imputed_training=fast_knn(train.values, k=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C27JsqcrpZ_w"
      },
      "source": [
        "**Categorical Imputation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNLK5kuZmdAf"
      },
      "source": [
        "For missing data in a categorical column, use the following method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukSfT4BkrElf"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sebom1snmd6-"
      },
      "source": [
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtf-tOcdmybY"
      },
      "source": [
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTXug85JoiTh"
      },
      "source": [
        "For this exercise - remove some of the categorical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XX3WIxqoqLz"
      },
      "source": [
        "column = dataset['Origin']\n",
        "missing_pct = int(column.size * 0.8)\n",
        "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
        "column[i] = np.NaN\n",
        "print(column.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsiXl8EGoyuA"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jggAq93crMOe"
      },
      "source": [
        "Fill the dataset with the most frequent value in that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Re070sIpaJ8"
      },
      "source": [
        "dataset = dataset.fillna(dataset.mode().iloc[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1KsGs6po2r0"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ufyTqfc4G7"
      },
      "source": [
        "**Imputation Using Multivariate Imputation by Chained Equation (MICE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ8n52IOdHlM"
      },
      "source": [
        "This type of imputation works by filling the missing data multiple times. Multiple Imputations (MIs) are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. For more information on the algorithm mechanics, you can refer to the Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYMzPs5oc8oD"
      },
      "source": [
        "from impyute.imputation.cs import mice\n",
        "\n",
        "# start the MICE training\n",
        "imputed_training=mice(train.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95v6kouudOHH"
      },
      "source": [
        "**Imputation Using Deep Learning (Datawig):**\n",
        "This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. It also supports both CPU and GPU for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTTPOU9gn5s"
      },
      "source": [
        "Pros:\n",
        "Quite accurate compared to other methods.\n",
        "It has some functions that can handle categorical data (Feature Encoder).\n",
        "It supports CPUs and GPUs.\n",
        "Cons:\n",
        "Single Column imputation.\n",
        "Can be quite slow with large datasets.\n",
        "You have to specify the columns that contain information about the target column that will be imputed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXc1bje26Qlo"
      },
      "source": [
        "!pip3 install datawig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y15kZkaydTUk"
      },
      "source": [
        "import datawig\n",
        "\n",
        "df_train, df_test = datawig.utils.random_split(train)\n",
        "\n",
        "#Initialize a SimpleImputer model\n",
        "imputer = datawig.SimpleImputer(\n",
        "    input_columns=['1','2','3','4','5','6','7', 'target'], # column(s) containing information about the column we want to impute\n",
        "    output_column= '0', # the column we'd like to impute values for\n",
        "    output_path = 'imputer_model' # stores model data and metrics\n",
        "    )\n",
        "\n",
        "#Fit an imputer model on the train data\n",
        "imputer.fit(train_df=df_train, num_epochs=50)\n",
        "\n",
        "#Impute missing values and return original dataframe with predictions\n",
        "imputed = imputer.predict(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqi39GZppgXW"
      },
      "source": [
        "Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHt4WjZ6pvFC"
      },
      "source": [
        "Detect Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-iO3202pgha"
      },
      "source": [
        "#Dropping the outlier rows with standard deviation\n",
        "factor = 3\n",
        "upper_lim = data['0'].mean() + data['0'].std () * factor\n",
        "lower_lim = data['0'].mean () - data['0'].std () * factor\n",
        "\n",
        "data = data[(data['0'] < upper_lim) & (data['0'] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72MJqFCLpxLA"
      },
      "source": [
        "Dropping Outlier Rows with Standard Deviation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4mi-eVYp-fa"
      },
      "source": [
        "#Dropping the outlier rows with standard deviation\n",
        "factor = 3\n",
        "upper_lim = data['0'].mean () + data['0'].std () * factor\n",
        "lower_lim = data['0'].mean () - data['0'].std () * factor\n",
        "\n",
        "data = data[(data['0'] < upper_lim) & (data['0'] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGXOLUqOp_Ea"
      },
      "source": [
        "Outlier Detection with Percentiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcU2zmHkpxWs"
      },
      "source": [
        "#Dropping the outlier rows with Percentiles\n",
        "upper_lim = data['0'].quantile(.95)\n",
        "lower_lim = data['0'].quantile(.05)\n",
        "\n",
        "data = data[(data['0'] < upper_lim) & (data['0'] > lower_lim)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg9oYcqsqGeQ"
      },
      "source": [
        "An Outlier Dilemma: Drop or Cap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9BAjhigqHEi"
      },
      "source": [
        "#Capping the outlier rows with Percentiles\n",
        "upper_lim = data['0'].quantile(.95)\n",
        "lower_lim = data['0'].quantile(.05)\n",
        "data.loc[(df['0'] > upper_lim),column] = upper_lim\n",
        "data.loc[(df['0'] < lower_lim),column] = lower_lim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyTbLsbNxhEe"
      },
      "source": [
        "# **Binning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8w3s9qRhW13"
      },
      "source": [
        "A column of continuous numbers has too many unique values to model effectively, so you automatically or manually assign the values to groups, to create a smaller set of discrete ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcWB_7lBhcLn"
      },
      "source": [
        "Replace a column of numbers with categorical values that represent specific ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD00G-47hhvm"
      },
      "source": [
        "A dataset has a few extreme values, all well outside the expected range, and these values have an outsized influence on the trained model. To mitigate the bias in the model, you might transform the data to a uniform distribution, using the quantiles (or equal-height) method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNcscP8zMrQ"
      },
      "source": [
        "insert image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z031fi5tzPF7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvSjeWTqzVwh"
      },
      "source": [
        "Binning can be applied on both categorical and numerical data:\n",
        "Numerical Binning Example\n",
        "Value      Bin       \n",
        "0-30   ->  Low       \n",
        "31-70  ->  Mid       \n",
        "71-100 ->  High\n",
        "\n",
        "Categorical Binning Example\n",
        "Value      Bin       \n",
        "Spain  ->  Europe      \n",
        "Italy  ->  Europe       \n",
        "Chile  ->  South America\n",
        "Brazil ->  South America"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxHeug8zfcL"
      },
      "source": [
        "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance.  Every time you bin something, you sacrifice information **and make your data more regularized.**\n",
        "\n",
        "The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.\n",
        "However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-x7EPoTz5nF"
      },
      "source": [
        "binning example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD6Ch-xV0Gx8"
      },
      "source": [
        "Log Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cUvALCb0T1s"
      },
      "source": [
        "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:\n",
        "It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
        "In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.\n",
        "It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.\n",
        "A critical note: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.\n",
        "Log(x+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZuX78s0Z46"
      },
      "source": [
        "#Log Transform Example\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
        "data['log+1'] = (data['value']+1).transform(np.log)\n",
        "#Negative Values Handling\n",
        "#Note that the values are different\n",
        "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
        "   value  log(x+1)  log(x-min(x)+1)\n",
        "0      2   1.09861          3.25810\n",
        "1     45   3.82864          4.23411\n",
        "2    -23       nan          0.00000\n",
        "3     85   4.45435          4.69135\n",
        "4     28   3.36730          3.95124\n",
        "5      2   1.09861          3.25810\n",
        "6     35   3.58352          4.07754\n",
        "7    -12       nan          2.48491"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6VxT_3I0hkO"
      },
      "source": [
        "One-hot encoding\n",
        "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
        "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0A5UuWT0pqd"
      },
      "source": [
        "Grouping Operations\n",
        "In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called “Tidy”.\n",
        "Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\n",
        "— Hadley Wickham\n",
        "Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.\n",
        "The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCfV7Szq0vZy"
      },
      "source": [
        "Categorical Column Grouping\n",
        "I suggest three different ways for aggregating categorical columns:\n",
        "The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\n",
        "data.groupby('id').agg(lambda x: x.value_counts().index[0])\n",
        "Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.\n",
        "\n",
        "Pivot table example: Sum of Visit Days grouped by Users\n",
        "#Pivot table Pandas Example\n",
        "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)\n",
        "Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping.\n",
        "Numerical Column Grouping\n",
        "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either.\n",
        "#sum_cols: List of columns to sum\n",
        "#mean_cols: List of columns to average\n",
        "grouped = data.groupby('column_to_group')\n",
        "\n",
        "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
        "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
        "\n",
        "new_df = pd.concat([sums, avgs], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRPilN6K00XT"
      },
      "source": [
        "Feature Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv4YQeoY05Mo"
      },
      "source": [
        "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\n",
        "We enable machine learning algorithms to comprehend them.\n",
        "Make possible to bin and group them.\n",
        "Improve model performance by uncovering potential information.\n",
        "Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:\n",
        "data.name\n",
        "0  Luther N. Gonzalez\n",
        "1    Charles M. Young\n",
        "2        Terry Lawson\n",
        "3       Kristen White\n",
        "4      Thomas Logsdon\n",
        "#Extracting first names\n",
        "data.name.str.split(\" \").map(lambda x: x[0])\n",
        "0     Luther\n",
        "1    Charles\n",
        "2      Terry\n",
        "3    Kristen\n",
        "4     Thomas\n",
        "#Extracting last names\n",
        "data.name.str.split(\" \").map(lambda x: x[-1])\n",
        "0    Gonzalez\n",
        "1       Young\n",
        "2      Lawson\n",
        "3       White\n",
        "4     Logsdon\n",
        "The example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.\n",
        "Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row.\n",
        "#String extraction example\n",
        "data.title.head()\n",
        "0                      Toy Story (1995)\n",
        "1                        Jumanji (1995)\n",
        "2               Grumpier Old Men (1995)\n",
        "3              Waiting to Exhale (1995)\n",
        "4    Father of the Bride Part II (1995)\n",
        "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
        "0    1995\n",
        "1    1995\n",
        "2    1995\n",
        "3    1995\n",
        "4    1995"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA8p523_08ka"
      },
      "source": [
        "Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCWnmjWv09il"
      },
      "source": [
        "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?\n",
        "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.\n",
        "Basically, there are two common ways of scaling:\n",
        "Normalization\n",
        "\n",
        "Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
        "\n",
        "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())\n",
        "   value  normalized\n",
        "0      2        0.23\n",
        "1     45        0.63\n",
        "2    -23        0.00\n",
        "3     85        1.00\n",
        "4     28        0.47\n",
        "5      2        0.23\n",
        "6     35        0.54\n",
        "7    -12        0.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJhvMQR11JgY"
      },
      "source": [
        "Standardization\n",
        "Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.\n",
        "In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ.\n",
        "\n",
        "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
        "\n",
        "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
        "   value  standardized\n",
        "0      2         -0.52\n",
        "1     45          0.70\n",
        "2    -23         -1.23\n",
        "3     85          1.84\n",
        "4     28          0.22\n",
        "5      2         -0.52\n",
        "6     35          0.42\n",
        "7    -12         -0.92"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV3sq4cm1P3-"
      },
      "source": [
        "Extracting Date\n",
        "Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like \"01–01–2017\".\n",
        "Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:\n",
        "Extracting the parts of the date into different columns: Year, month, day, etc.\n",
        "Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
        "Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\n",
        "If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them.\n",
        "from datetime import date\n",
        "\n",
        "data = pd.DataFrame({'date':\n",
        "['01-01-2017',\n",
        "'04-12-2008',\n",
        "'23-06-1988',\n",
        "'25-08-1999',\n",
        "'20-02-1993',\n",
        "]})\n",
        "\n",
        "#Transform string to date\n",
        "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
        "\n",
        "#Extracting Year\n",
        "data['year'] = data['date'].dt.year\n",
        "\n",
        "#Extracting Month\n",
        "data['month'] = data['date'].dt.month\n",
        "\n",
        "#Extracting passed years since the date\n",
        "data['passed_years'] = date.today().year - data['date'].dt.year\n",
        "\n",
        "#Extracting passed months since the date\n",
        "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
        "\n",
        "#Extracting the weekday name of the date\n",
        "data['day_name'] = data['date'].dt.day_name()\n",
        "        date  year  month  passed_years  passed_months   day_name\n",
        "0 2017-01-01  2017      1             2             26     Sunday\n",
        "1 2008-12-04  2008     12            11            123   Thursday\n",
        "2 1988-06-23  1988      6            31            369   Thursday\n",
        "3 1999-08-25  1999      8            20            235  Wednesday\n",
        "4 1993-02-20  1993      2            26            313   Saturday"
      ]
    }
  ]
}